"""
DevDesk-RAG ê³ ê¸‰ ê²€ìƒ‰ ì•Œê³ ë¦¬ì¦˜
ë™ì  ê°€ì¤‘ì¹˜ ì¡°ì •, A/B í…ŒìŠ¤íŠ¸, ì„±ëŠ¥ ë³‘ëª© ë¶„ì„, ìë™ ìµœì í™”
"""

import time
import json
import sqlite3
from datetime import datetime, timedelta
from typing import Dict, List, Optional, Any, Tuple
from dataclasses import dataclass, asdict
from collections import defaultdict, deque
import threading
import logging
import random
import numpy as np
from enum import Enum

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class SearchAlgorithm(Enum):
    """ê²€ìƒ‰ ì•Œê³ ë¦¬ì¦˜ ìœ í˜•"""
    VECTOR_ONLY = "vector_only"           # ë²¡í„° ê²€ìƒ‰ë§Œ
    BM25_ONLY = "bm25_only"              # BM25ë§Œ
    HYBRID = "hybrid"                     # í•˜ì´ë¸Œë¦¬ë“œ (ë²¡í„° + BM25)
    WEIGHTED_HYBRID = "weighted_hybrid"  # ê°€ì¤‘ì¹˜ ê¸°ë°˜ í•˜ì´ë¸Œë¦¬ë“œ
    ADAPTIVE = "adaptive"                 # ì ì‘í˜• ê°€ì¤‘ì¹˜

class SearchResult:
    """ê²€ìƒ‰ ê²°ê³¼ ë°ì´í„° í´ë˜ìŠ¤"""
    
    def __init__(self, content: str, metadata: Dict[str, Any], score: float, source: str):
        self.content = content
        self.metadata = metadata
        self.score = score
        self.source = source
        self.rank = 0
        self.relevance_score = 0.0
    
    def to_dict(self) -> Dict[str, Any]:
        return {
            'content': self.content,
            'metadata': self.metadata,
            'score': self.score,
            'source': self.source,
            'rank': self.rank,
            'relevance_score': self.relevance_score
        }

@dataclass
class SearchMetrics:
    """ê²€ìƒ‰ ì„±ëŠ¥ ë©”íŠ¸ë¦­"""
    timestamp: float
    query: str
    algorithm: str
    search_time: float
    results_count: int
    avg_score: float
    top_score: float
    user_feedback: Optional[float] = None
    relevance_score: float = 0.0
    
    def __post_init__(self):
        if self.timestamp is None:
            self.timestamp = time.time()

class WeightOptimizer:
    """ë™ì  ê°€ì¤‘ì¹˜ ìµœì í™” ì‹œìŠ¤í…œ"""
    
    def __init__(self):
        self.weights_history = deque(maxlen=1000)
        self.performance_history = deque(maxlen=1000)
        self.current_weights = {
            'vector_weight': 0.7,
            'bm25_weight': 0.3,
            'rerank_weight': 0.5
        }
        self.learning_rate = 0.01
        self.exploration_rate = 0.1
    
    def update_weights(self, performance_metrics: List[SearchMetrics]) -> Dict[str, float]:
        """ì„±ëŠ¥ ë©”íŠ¸ë¦­ì„ ê¸°ë°˜ìœ¼ë¡œ ê°€ì¤‘ì¹˜ ì—…ë°ì´íŠ¸"""
        if not performance_metrics:
            return self.current_weights
        
        # ìµœê·¼ ì„±ëŠ¥ ê³„ì‚°
        recent_metrics = performance_metrics[-100:]  # ìµœê·¼ 100ê°œ
        avg_relevance = np.mean([m.relevance_score for m in recent_metrics if m.relevance_score > 0])
        avg_search_time = np.mean([m.search_time for m in recent_metrics])
        
        # ê°€ì¤‘ì¹˜ ì¡°ì • ë°©í–¥ ê²°ì •
        if avg_relevance > 0.8:  # ë†’ì€ ê´€ë ¨ì„±
            # í˜„ì¬ ê°€ì¤‘ì¹˜ ìœ ì§€í•˜ë©´ì„œ ë¯¸ì„¸ ì¡°ì •
            self.current_weights['vector_weight'] += self.learning_rate * 0.1
            self.current_weights['bm25_weight'] -= self.learning_rate * 0.1
        elif avg_relevance < 0.6:  # ë‚®ì€ ê´€ë ¨ì„±
            # ë” ê· í˜•ì¡íŒ ê°€ì¤‘ì¹˜ë¡œ ì¡°ì •
            self.current_weights['vector_weight'] = 0.6
            self.current_weights['bm25_weight'] = 0.4
        
        # íƒìƒ‰ì  ì¡°ì • (10% í™•ë¥ )
        if random.random() < self.exploration_rate:
            self._explore_new_weights()
        
        # ê°€ì¤‘ì¹˜ ì •ê·œí™”
        total_weight = sum(self.current_weights.values())
        self.current_weights = {k: v/total_weight for k, v in self.current_weights.items()}
        
        # íˆìŠ¤í† ë¦¬ ì €ì¥
        self.weights_history.append({
            'timestamp': time.time(),
            'weights': self.current_weights.copy(),
            'performance': {'relevance': avg_relevance, 'search_time': avg_search_time}
        })
        
        return self.current_weights
    
    def _explore_new_weights(self):
        """ìƒˆë¡œìš´ ê°€ì¤‘ì¹˜ ì¡°í•© íƒìƒ‰"""
        # ê°€ìš°ì‹œì•ˆ ë…¸ì´ì¦ˆ ì¶”ê°€
        noise = np.random.normal(0, 0.05)
        self.current_weights['vector_weight'] += noise
        self.current_weights['bm25_weight'] -= noise
        
        # ê²½ê³„ ì œí•œ
        self.current_weights['vector_weight'] = max(0.3, min(0.9, self.current_weights['vector_weight']))
        self.current_weights['bm25_weight'] = max(0.1, min(0.7, self.current_weights['bm25_weight']))

class ABTestFramework:
    """A/B í…ŒìŠ¤íŠ¸ í”„ë ˆì„ì›Œí¬"""
    
    def __init__(self):
        self.experiments = {}
        self.user_assignments = {}
        self.results = defaultdict(list)
    
    def create_experiment(self, experiment_id: str, variants: List[Dict[str, Any]], 
                         traffic_split: List[float] = None) -> bool:
        """ìƒˆë¡œìš´ A/B í…ŒìŠ¤íŠ¸ ì‹¤í—˜ ìƒì„±"""
        try:
            if traffic_split is None:
                traffic_split = [1.0 / len(variants)] * len(variants)
            
            if abs(sum(traffic_split) - 1.0) > 0.01:
                raise ValueError("Traffic split must sum to 1.0")
            
            self.experiments[experiment_id] = {
                'variants': variants,
                'traffic_split': traffic_split,
                'start_time': time.time(),
                'status': 'active'
            }
            
            logger.info(f"A/B í…ŒìŠ¤íŠ¸ ì‹¤í—˜ ìƒì„±: {experiment_id}")
            return True
            
        except Exception as e:
            logger.error(f"A/B í…ŒìŠ¤íŠ¸ ì‹¤í—˜ ìƒì„± ì‹¤íŒ¨: {e}")
            return False
    
    def get_variant(self, experiment_id: str, user_id: str) -> Optional[Dict[str, Any]]:
        """ì‚¬ìš©ìì—ê²Œ í…ŒìŠ¤íŠ¸ ë³€í˜• í• ë‹¹"""
        if experiment_id not in self.experiments:
            return None
        
        experiment = self.experiments[experiment_id]
        if experiment['status'] != 'active':
            return None
        
        # ì‚¬ìš©ì í• ë‹¹ í™•ì¸
        if user_id not in self.user_assignments:
            # ìƒˆë¡œìš´ ì‚¬ìš©ì í• ë‹¹
            variant_index = self._assign_variant(experiment['traffic_split'])
            self.user_assignments[user_id] = variant_index
        
        variant_index = self.user_assignments[user_id]
        return experiment['variants'][variant_index]
    
    def _assign_variant(self, traffic_split: List[float]) -> int:
        """íŠ¸ë˜í”½ ë¶„í• ì— ë”°ë¥¸ ë³€í˜• í• ë‹¹"""
        rand = random.random()
        cumulative = 0
        
        for i, split in enumerate(traffic_split):
            cumulative += split
            if rand <= cumulative:
                return i
        
        return len(traffic_split) - 1
    
    def record_result(self, experiment_id: str, user_id: str, variant_index: int, 
                     metrics: Dict[str, Any]) -> bool:
        """A/B í…ŒìŠ¤íŠ¸ ê²°ê³¼ ê¸°ë¡"""
        try:
            result = {
                'timestamp': time.time(),
                'user_id': user_id,
                'variant_index': variant_index,
                'metrics': metrics
            }
            
            self.results[experiment_id].append(result)
            logger.debug(f"A/B í…ŒìŠ¤íŠ¸ ê²°ê³¼ ê¸°ë¡: {experiment_id} - {user_id}")
            return True
            
        except Exception as e:
            logger.error(f"A/B í…ŒìŠ¤íŠ¸ ê²°ê³¼ ê¸°ë¡ ì‹¤íŒ¨: {e}")
            return False
    
    def get_experiment_results(self, experiment_id: str) -> Dict[str, Any]:
        """A/B í…ŒìŠ¤íŠ¸ ê²°ê³¼ ë¶„ì„"""
        if experiment_id not in self.results:
            return {}
        
        results = self.results[experiment_id]
        if not results:
            return {}
        
        # ë³€í˜•ë³„ ì„±ëŠ¥ ë¶„ì„
        variant_performance = defaultdict(list)
        for result in results:
            variant_index = result['variant_index']
            variant_performance[variant_index].append(result['metrics'])
        
        analysis = {}
        for variant_index, metrics_list in variant_performance.items():
            if metrics_list:
                # í‰ê·  ì„±ëŠ¥ ê³„ì‚°
                avg_relevance = np.mean([m.get('relevance_score', 0) for m in metrics_list])
                avg_search_time = np.mean([m.get('search_time', 0) for m in metrics_list])
                avg_user_feedback = np.mean([m.get('user_feedback', 0) for m in metrics_list if m.get('user_feedback')])
                
                analysis[f'variant_{variant_index}'] = {
                    'count': len(metrics_list),
                    'avg_relevance': round(avg_relevance, 3),
                    'avg_search_time': round(avg_search_time, 3),
                    'avg_user_feedback': round(avg_user_feedback, 3) if avg_user_feedback > 0 else None
                }
        
        return analysis

class PerformanceAnalyzer:
    """ì„±ëŠ¥ ë³‘ëª© ì§€ì  ë¶„ì„ ë„êµ¬"""
    
    def __init__(self):
        self.bottleneck_history = deque(maxlen=1000)
        self.thresholds = {
            'search_time': 1.0,      # 1ì´ˆ ì´ìƒì´ë©´ ë³‘ëª©
            'relevance_score': 0.6,   # 0.6 ì´í•˜ë©´ ë‚®ì€ í’ˆì§ˆ
            'results_count': 2        # 2ê°œ ì´í•˜ë©´ ë¶€ì¡±í•œ ê²°ê³¼
        }
    
    def analyze_bottleneck(self, metrics: SearchMetrics) -> Dict[str, Any]:
        """ì„±ëŠ¥ ë³‘ëª© ì§€ì  ë¶„ì„"""
        bottlenecks = []
        
        # ê²€ìƒ‰ ì‹œê°„ ë³‘ëª©
        if metrics.search_time > self.thresholds['search_time']:
            bottlenecks.append({
                'type': 'search_time',
                'severity': 'high' if metrics.search_time > 2.0 else 'medium',
                'value': metrics.search_time,
                'threshold': self.thresholds['search_time'],
                'suggestion': 'ê²€ìƒ‰ ì•Œê³ ë¦¬ì¦˜ ìµœì í™” ë˜ëŠ” ì¸ë±ìŠ¤ ê°œì„  í•„ìš”'
            })
        
        # ê´€ë ¨ì„± ì ìˆ˜ ë³‘ëª©
        if metrics.relevance_score < self.thresholds['relevance_score']:
            bottlenecks.append({
                'type': 'relevance_score',
                'severity': 'high' if metrics.relevance_score < 0.4 else 'medium',
                'value': metrics.relevance_score,
                'threshold': self.thresholds['relevance_score'],
                'suggestion': 'ì„ë² ë”© ëª¨ë¸ ê°œì„  ë˜ëŠ” ê²€ìƒ‰ íŒŒë¼ë¯¸í„° ì¡°ì • í•„ìš”'
            })
        
        # ê²°ê³¼ ìˆ˜ ë³‘ëª©
        if metrics.results_count < self.thresholds['results_count']:
            bottlenecks.append({
                'type': 'results_count',
                'severity': 'medium',
                'value': metrics.results_count,
                'threshold': self.thresholds['results_count'],
                'suggestion': 'ê²€ìƒ‰ ë²”ìœ„ í™•ëŒ€ ë˜ëŠ” ì²­í‚¹ ì „ëµ ê°œì„  í•„ìš”'
            })
        
        # ë³‘ëª© íˆìŠ¤í† ë¦¬ ì €ì¥
        if bottlenecks:
            self.bottleneck_history.append({
                'timestamp': metrics.timestamp,
                'query': metrics.query,
                'algorithm': metrics.algorithm,
                'bottlenecks': bottlenecks
            })
        
        return {
            'has_bottleneck': len(bottlenecks) > 0,
            'bottlenecks': bottlenecks,
            'overall_severity': self._calculate_overall_severity(bottlenecks)
        }
    
    def _calculate_overall_severity(self, bottlenecks: List[Dict[str, Any]]) -> str:
        """ì „ì²´ ë³‘ëª© ì‹¬ê°ë„ ê³„ì‚°"""
        if not bottlenecks:
            return 'none'
        
        severity_scores = {'low': 1, 'medium': 2, 'high': 3}
        max_severity = max(severity_scores.get(b['severity'], 0) for b in bottlenecks)
        
        if max_severity >= 3:
            return 'critical'
        elif max_severity >= 2:
            return 'high'
        else:
            return 'low'
    
    def get_bottleneck_trends(self, hours: int = 24) -> Dict[str, Any]:
        """ë³‘ëª© íŠ¸ë Œë“œ ë¶„ì„"""
        cutoff_time = time.time() - (hours * 3600)
        recent_bottlenecks = [b for b in self.bottleneck_history if b['timestamp'] > cutoff_time]
        
        if not recent_bottlenecks:
            return {}
        
        # ë³‘ëª© ìœ í˜•ë³„ ë°œìƒ ë¹ˆë„
        bottleneck_types = defaultdict(int)
        algorithm_bottlenecks = defaultdict(int)
        
        for bottleneck in recent_bottlenecks:
            for b in bottleneck['bottlenecks']:
                bottleneck_types[b['type']] += 1
            algorithm_bottlenecks[bottleneck['algorithm']] += 1
        
        return {
            'total_bottlenecks': len(recent_bottlenecks),
            'bottleneck_types': dict(bottleneck_types),
            'algorithm_bottlenecks': dict(algorithm_bottlenecks),
            'trend': self._analyze_trend(recent_bottlenecks)
        }
    
    def _analyze_trend(self, bottlenecks: List[Dict[str, Any]]) -> str:
        """ë³‘ëª© íŠ¸ë Œë“œ ë¶„ì„"""
        if len(bottlenecks) < 2:
            return 'insufficient_data'
        
        # ì‹œê°„ìˆœìœ¼ë¡œ ì •ë ¬
        sorted_bottlenecks = sorted(bottlenecks, key=lambda x: x['timestamp'])
        
        # ìµœê·¼ 1/3ê³¼ ì´ì „ 1/3 ë¹„êµ
        third = len(sorted_bottlenecks) // 3
        recent = sorted_bottlenecks[-third:]
        previous = sorted_bottlenecks[:third]
        
        recent_count = len(recent)
        previous_count = len(previous)
        
        if recent_count < previous_count * 0.7:
            return 'improving'
        elif recent_count > previous_count * 1.3:
            return 'worsening'
        else:
            return 'stable'

class AdvancedSearchEngine:
    """ê³ ê¸‰ ê²€ìƒ‰ ì—”ì§„ - ë™ì  ê°€ì¤‘ì¹˜ ì¡°ì •, A/B í…ŒìŠ¤íŠ¸, ì„±ëŠ¥ ë³‘ëª© ë¶„ì„"""
    
    def __init__(self):
        self.weight_optimizer = WeightOptimizer()
        self.ab_test_framework = ABTestFramework()
        self.performance_analyzer = PerformanceAnalyzer()
        self.search_metrics_history = deque(maxlen=1000)
        
        # ì‹¤ì œ ê²€ìƒ‰ ì‹œìŠ¤í…œê³¼ì˜ ì—°ë™ì„ ìœ„í•œ ë³€ìˆ˜ë“¤
        self.vector_store = None
        self.retriever = None
        self.embedding_model = None
        self.bm25_retriever = None
        
        # ê²€ìƒ‰ ì„¤ì •
        self.search_config = {
            'max_results': 10,
            'min_score_threshold': 0.3,
            'rerank_enabled': True,
            'domain_weights': {
                'technical': {'vector': 0.8, 'bm25': 0.2},
                'general': {'vector': 0.6, 'bm25': 0.4},
                'code': {'vector': 0.7, 'bm25': 0.3}
            }
        }
        
        # ë„ë©”ì¸ ë¶„ë¥˜ê¸° (ê°„ë‹¨í•œ í‚¤ì›Œë“œ ê¸°ë°˜)
        self.domain_classifier = {
            'technical': ['api', 'system', 'performance', 'algorithm', 'optimization', 'architecture'],
            'general': ['what', 'how', 'explain', 'describe', 'introduction', 'overview'],
            'code': ['code', 'implementation', 'function', 'class', 'method', 'syntax', 'error']
        }
    
    def initialize_search_system(self, vector_store, retriever, embedding_model, bm25_retriever=None):
        """ì‹¤ì œ ê²€ìƒ‰ ì‹œìŠ¤í…œê³¼ ì—°ë™ ì´ˆê¸°í™”"""
        self.vector_store = vector_store
        self.retriever = retriever
        self.embedding_model = embedding_model
        self.bm25_retriever = bm25_retriever
        
        logger.info("ê³ ê¸‰ ê²€ìƒ‰ ì—”ì§„ ì´ˆê¸°í™” ì™„ë£Œ")
        logger.info(f"ë²¡í„° ìŠ¤í† ì–´: {type(vector_store).__name__}")
        logger.info(f"ë¦¬íŠ¸ë¦¬ë²„: {type(retriever).__name__}")
        logger.info(f"ì„ë² ë”© ëª¨ë¸: {type(embedding_model).__name__}")
        if bm25_retriever:
            logger.info(f"BM25 ë¦¬íŠ¸ë¦¬ë²„: {type(bm25_retriever).__name__}")
    
    def classify_query_domain(self, query: str) -> str:
        """ì¿¼ë¦¬ì˜ ë„ë©”ì¸ ë¶„ë¥˜"""
        query_lower = query.lower()
        
        for domain, keywords in self.domain_classifier.items():
            if any(keyword in query_lower for keyword in keywords):
                return domain
        
        return 'general'  # ê¸°ë³¸ê°’
    
    def get_domain_weights(self, domain: str) -> Dict[str, float]:
        """ë„ë©”ì¸ë³„ ê°€ì¤‘ì¹˜ ë°˜í™˜"""
        return self.search_config['domain_weights'].get(domain, self.search_config['domain_weights']['general'])
    
    def search(self, query: str, user_id: str = None, 
               algorithm: str = None, use_ab_test: bool = True) -> List[SearchResult]:
        """ê³ ê¸‰ ê²€ìƒ‰ ì‹¤í–‰"""
        start_time = time.time()
        
        # A/B í…ŒìŠ¤íŠ¸ ë³€í˜• í• ë‹¹
        if use_ab_test and user_id:
            variant = self.ab_test_framework.get_variant("search_algorithm_comparison", user_id)
            if variant:
                algorithm = variant['algorithm']
        
        # ì•Œê³ ë¦¬ì¦˜ë³„ ê²€ìƒ‰ ì‹¤í–‰
        if algorithm == 'vector_only':
            results = self._vector_search(query)
        elif algorithm == 'bm25_only':
            results = self._bm25_search(query)
        elif algorithm == 'hybrid':
            results = self._hybrid_search(query)
        elif algorithm == 'weighted_hybrid':
            results = self._weighted_hybrid_search(query)
        else:
            # ê¸°ë³¸ê°’: ê°€ì¤‘ì¹˜ ê¸°ë°˜ í•˜ì´ë¸Œë¦¬ë“œ
            results = self._weighted_hybrid_search(query)
        
        search_time = time.time() - start_time
        
        # ì„±ëŠ¥ ë©”íŠ¸ë¦­ ìƒì„±
        metrics = SearchMetrics(
            timestamp=start_time,
            query=query,
            algorithm=algorithm or 'weighted_hybrid',
            search_time=search_time,
            results_count=len(results),
            avg_score=np.mean([r.score for r in results]) if results else 0,
            top_score=results[0].score if results else 0
        )
        
        # ì„±ëŠ¥ ë¶„ì„
        bottleneck_analysis = self.performance_analyzer.analyze_bottleneck(metrics)
        
        # ë©”íŠ¸ë¦­ íˆìŠ¤í† ë¦¬ ì €ì¥
        self.search_metrics_history.append(metrics)
        
        # A/B í…ŒìŠ¤íŠ¸ ê²°ê³¼ ê¸°ë¡
        if user_id and use_ab_test:
            variant_index = self._get_variant_index(algorithm)
            self.ab_test_framework.record_result(
                "search_algorithm_comparison",
                user_id,
                variant_index,
                {
                    'search_time': search_time,
                    'results_count': len(results),
                    'avg_score': metrics.avg_score,
                    'top_score': metrics.top_score,
                    'bottleneck_severity': bottleneck_analysis['overall_severity']
                }
            )
        
        # ê°€ì¤‘ì¹˜ ìµœì í™”
        self.weight_optimizer.update_weights(list(self.search_metrics_history))
        
        return results
    
    def _vector_search(self, query: str) -> List[SearchResult]:
        """ì‹¤ì œ ë²¡í„° ê²€ìƒ‰ ì‹¤í–‰"""
        if not self.retriever:
            logger.warning("ë²¡í„° ë¦¬íŠ¸ë¦¬ë²„ê°€ ì´ˆê¸°í™”ë˜ì§€ ì•ŠìŒ")
            return []
        
        try:
            # ê¸°ì¡´ ë¦¬íŠ¸ë¦¬ë²„ë¥¼ ì‚¬ìš©í•˜ì—¬ ê²€ìƒ‰
            docs = self.retriever.invoke(query)
            
            # SearchResultë¡œ ë³€í™˜
            results = []
            for i, doc in enumerate(docs):
                # ê°„ë‹¨í•œ ì ìˆ˜ ê³„ì‚° (ìˆœìœ„ ê¸°ë°˜)
                score = max(0.1, 1.0 - (i * 0.1))
                
                result = SearchResult(
                    content=doc.page_content,
                    metadata=doc.metadata,
                    score=score,
                    source='vector_search'
                )
                result.rank = i + 1
                results.append(result)
            
            logger.debug(f"ë²¡í„° ê²€ìƒ‰ ì™„ë£Œ: {len(results)}ê°œ ê²°ê³¼")
            return results
            
        except Exception as e:
            logger.error(f"ë²¡í„° ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
            return []
    
    def _bm25_search(self, query: str) -> List[SearchResult]:
        """ì‹¤ì œ BM25 ê²€ìƒ‰ ì‹¤í–‰"""
        if not self.bm25_retriever:
            logger.warning("BM25 ë¦¬íŠ¸ë¦¬ë²„ê°€ ì´ˆê¸°í™”ë˜ì§€ ì•ŠìŒ")
            return []
        
        try:
            # BM25 ê²€ìƒ‰ ì‹¤í–‰
            docs = self.bm25_retriever.invoke(query)
            
            # SearchResultë¡œ ë³€í™˜
            results = []
            for i, doc in enumerate(docs):
                # BM25 ì ìˆ˜ë¥¼ ì •ê·œí™” (0.1 ~ 1.0)
                score = max(0.1, min(1.0, doc.metadata.get('score', 0.5)))
                
                result = SearchResult(
                    content=doc.page_content,
                    metadata=doc.metadata,
                    score=score,
                    source='bm25_search'
                )
                result.rank = i + 1
                results.append(result)
            
            logger.debug(f"BM25 ê²€ìƒ‰ ì™„ë£Œ: {len(results)}ê°œ ê²°ê³¼")
            return results
            
        except Exception as e:
            logger.error(f"BM25 ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
            return []
    
    def _hybrid_search(self, query: str) -> List[SearchResult]:
        """í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰"""
        vector_results = self._vector_search(query)
        bm25_results = self._bm25_search(query)
        
        # ê²°ê³¼ ë³‘í•© ë° ì¬ë­í‚¹
        all_results = vector_results + bm25_results
        all_results.sort(key=lambda x: x.score, reverse=True)
        
        return all_results[:5]  # ìƒìœ„ 5ê°œ ê²°ê³¼
    
    def _weighted_hybrid_search(self, query: str) -> List[SearchResult]:
        """ë„ë©”ì¸ë³„ ê°€ì¤‘ì¹˜ ê¸°ë°˜ í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰"""
        # ë„ë©”ì¸ ë¶„ë¥˜ ë° ê°€ì¤‘ì¹˜ ê²°ì •
        domain = self.classify_query_domain(query)
        domain_weights = self.get_domain_weights(domain)
        
        logger.debug(f"ì¿¼ë¦¬ ë„ë©”ì¸: {domain}, ê°€ì¤‘ì¹˜: {domain_weights}")
        
        # ê° ê²€ìƒ‰ ì•Œê³ ë¦¬ì¦˜ ì‹¤í–‰
        vector_results = self._vector_search(query)
        bm25_results = self._bm25_search(query) if self.bm25_retriever else []
        
        # ë„ë©”ì¸ë³„ ê°€ì¤‘ì¹˜ ì ìš©
        for result in vector_results:
            result.score *= domain_weights['vector']
            result.relevance_score = result.score  # ê´€ë ¨ì„± ì ìˆ˜ ì„¤ì •
        
        for result in bm25_results:
            result.score *= domain_weights['bm25']
            result.relevance_score = result.score  # ê´€ë ¨ì„± ì ìˆ˜ ì„¤ì •
        
        # ê²°ê³¼ ë³‘í•© ë° ì¤‘ë³µ ì œê±°
        all_results = self._merge_and_deduplicate_results(vector_results, bm25_results)
        
        # ìµœì¢… ì ìˆ˜ ê³„ì‚° ë° ì¬ë­í‚¹
        all_results = self._calculate_final_scores(all_results, query)
        all_results.sort(key=lambda x: x.score, reverse=True)
        
        # ìµœëŒ€ ê²°ê³¼ ìˆ˜ ì œí•œ
        max_results = self.search_config['max_results']
        return all_results[:max_results]
    
    def _merge_and_deduplicate_results(self, vector_results: List[SearchResult], 
                                     bm25_results: List[SearchResult]) -> List[SearchResult]:
        """ê²€ìƒ‰ ê²°ê³¼ ë³‘í•© ë° ì¤‘ë³µ ì œê±°"""
        merged_results = {}
        
        # ë²¡í„° ê²€ìƒ‰ ê²°ê³¼ ì¶”ê°€
        for result in vector_results:
            content_key = result.content[:100]  # ë‚´ìš©ì˜ ì²« 100ìë¡œ í‚¤ ìƒì„±
            if content_key not in merged_results:
                merged_results[content_key] = result
            else:
                # ë” ë†’ì€ ì ìˆ˜ë¡œ ì—…ë°ì´íŠ¸
                if result.score > merged_results[content_key].score:
                    merged_results[content_key] = result
        
        # BM25 ê²€ìƒ‰ ê²°ê³¼ ì¶”ê°€/ë³‘í•©
        for result in bm25_results:
            content_key = result.content[:100]
            if content_key not in merged_results:
                merged_results[content_key] = result
            else:
                # ê¸°ì¡´ ê²°ê³¼ì™€ ì ìˆ˜ ë³‘í•©
                existing = merged_results[content_key]
                existing.score = (existing.score + result.score) / 2
                existing.relevance_score = max(existing.relevance_score, result.relevance_score)
        
        return list(merged_results.values())
    
    def _calculate_final_scores(self, results: List[SearchResult], query: str) -> List[SearchResult]:
        """ìµœì¢… ì ìˆ˜ ê³„ì‚° (ì»¨í…ìŠ¤íŠ¸ ë° í’ˆì§ˆ ê³ ë ¤)"""
        for result in results:
            # ê¸°ë³¸ ì ìˆ˜
            base_score = result.score
            
            # ì»¨í…ìŠ¤íŠ¸ í’ˆì§ˆ ì ìˆ˜ (ë©”íƒ€ë°ì´í„° ê¸°ë°˜)
            context_score = self._calculate_context_score(result, query)
            
            # ìµœì¢… ì ìˆ˜ ê³„ì‚° (ê°€ì¤‘ í‰ê· )
            final_score = (base_score * 0.7) + (context_score * 0.3)
            result.score = min(1.0, max(0.1, final_score))
            
            # ê´€ë ¨ì„± ì ìˆ˜ ì—…ë°ì´íŠ¸
            result.relevance_score = result.score
        
        return results
    
    def _calculate_context_score(self, result: SearchResult, query: str) -> float:
        """ì»¨í…ìŠ¤íŠ¸ í’ˆì§ˆ ì ìˆ˜ ê³„ì‚°"""
        score = 0.5  # ê¸°ë³¸ ì ìˆ˜
        
        # ë©”íƒ€ë°ì´í„° í’ˆì§ˆ í‰ê°€
        metadata = result.metadata
        
        # ì†ŒìŠ¤ íŒŒì¼ ìœ í˜•
        if 'source' in metadata:
            source = metadata['source'].lower()
            if source.endswith('.md') or source.endswith('.txt'):
                score += 0.1  # ë§ˆí¬ë‹¤ìš´/í…ìŠ¤íŠ¸ íŒŒì¼ ìš°ì„ 
            elif source.endswith('.pdf'):
                score += 0.05  # PDF íŒŒì¼
        
        # ì²­í¬ ê¸¸ì´ (ì ì ˆí•œ ê¸¸ì´ ìš°ì„ )
        content_length = len(result.content)
        if 100 <= content_length <= 1000:
            score += 0.1  # ì ì ˆí•œ ê¸¸ì´
        elif content_length > 1000:
            score += 0.05  # ê¸´ ë‚´ìš©
        
        # ì¿¼ë¦¬ì™€ì˜ í‚¤ì›Œë“œ ë§¤ì¹­
        query_words = set(query.lower().split())
        content_words = set(result.content.lower().split())
        keyword_overlap = len(query_words.intersection(content_words))
        if keyword_overlap > 0:
            score += min(0.2, keyword_overlap * 0.05)
        
        return min(1.0, score)
    
    def _get_variant_index(self, algorithm: str) -> int:
        """ì•Œê³ ë¦¬ì¦˜ì— ë”°ë¥¸ ë³€í˜• ì¸ë±ìŠ¤ ë°˜í™˜"""
        algorithm_mapping = {
            'vector_only': 0,
            'hybrid': 1,
            'weighted_hybrid': 2
        }
        return algorithm_mapping.get(algorithm, 2)
    
    def get_performance_insights(self) -> Dict[str, Any]:
        """ì„±ëŠ¥ ì¸ì‚¬ì´íŠ¸ ì œê³µ"""
        return {
            'current_weights': self.weight_optimizer.current_weights,
            'bottleneck_trends': self.performance_analyzer.get_bottleneck_trends(),
            'ab_test_results': self.ab_test_framework.get_experiment_results("search_algorithm_comparison"),
            'search_metrics_summary': self._get_metrics_summary()
        }
    
    def _get_metrics_summary(self) -> Dict[str, Any]:
        """ê²€ìƒ‰ ë©”íŠ¸ë¦­ ìš”ì•½"""
        if not self.search_metrics_history:
            return {}
        
        recent_metrics = list(self.search_metrics_history)[-100:]  # ìµœê·¼ 100ê°œ
        
        return {
            'total_searches': len(self.search_metrics_history),
            'recent_searches': len(recent_metrics),
            'avg_search_time': np.mean([m.search_time for m in recent_metrics]),
            'avg_results_count': np.mean([m.results_count for m in recent_metrics]),
            'algorithm_distribution': self._get_algorithm_distribution(recent_metrics)
        }
    
    def _get_algorithm_distribution(self, metrics: List[SearchMetrics]) -> Dict[str, int]:
        """ì•Œê³ ë¦¬ì¦˜ë³„ ì‚¬ìš© ë¶„í¬"""
        distribution = defaultdict(int)
        for metric in metrics:
            distribution[metric.algorithm] += 1
        return dict(distribution)

# ì „ì—­ ê³ ê¸‰ ê²€ìƒ‰ ì—”ì§„ ì¸ìŠ¤í„´ìŠ¤
advanced_search_engine = AdvancedSearchEngine()

def get_search_insights() -> Dict[str, Any]:
    """ê²€ìƒ‰ ì„±ëŠ¥ ì¸ì‚¬ì´íŠ¸ ì¡°íšŒ"""
    return advanced_search_engine.get_performance_insights()

if __name__ == "__main__":
    # í…ŒìŠ¤íŠ¸ ì½”ë“œ
    print("ğŸ” DevDesk-RAG ê³ ê¸‰ ê²€ìƒ‰ ì•Œê³ ë¦¬ì¦˜ í…ŒìŠ¤íŠ¸")
    
    # ê²€ìƒ‰ í…ŒìŠ¤íŠ¸
    results = advanced_search_engine.search(
        query="DevDesk-RAG ì‹œìŠ¤í…œì˜ ì„±ëŠ¥ì€ ì–´ë–¨ê¹Œìš”?",
        user_id="test_user_123",
        algorithm="weighted_hybrid"
    )
    
    print(f"âœ… ê²€ìƒ‰ ì™„ë£Œ: {len(results)}ê°œ ê²°ê³¼")
    
    # ì„±ëŠ¥ ì¸ì‚¬ì´íŠ¸ ì¡°íšŒ
    insights = get_search_insights()
    print(f"ğŸ“Š ì„±ëŠ¥ ì¸ì‚¬ì´íŠ¸: {insights}")
    
    print("ğŸ‰ ê³ ê¸‰ ê²€ìƒ‰ ì•Œê³ ë¦¬ì¦˜ í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")
