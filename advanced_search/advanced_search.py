"""
DevDesk-RAG ê³ ê¸‰ ê²€ìƒ‰ ì•Œê³ ë¦¬ì¦˜
ë™ì  ê°€ì¤‘ì¹˜ ì¡°ì •, A/B í…ŒìŠ¤íŠ¸, ì„±ëŠ¥ ë³‘ëª© ë¶„ì„, ìë™ ìµœì í™”
"""

import time
import json
import sqlite3
from datetime import datetime, timedelta
from typing import Dict, List, Optional, Any, Tuple
from dataclasses import dataclass, asdict
from collections import defaultdict, deque
import threading
import logging
import random
import numpy as np
from enum import Enum

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class SearchAlgorithm(Enum):
    """ê²€ìƒ‰ ì•Œê³ ë¦¬ì¦˜ ìœ í˜•"""
    VECTOR_ONLY = "vector_only"           # ë²¡í„° ê²€ìƒ‰ë§Œ
    BM25_ONLY = "bm25_only"              # BM25ë§Œ
    HYBRID = "hybrid"                     # í•˜ì´ë¸Œë¦¬ë“œ (ë²¡í„° + BM25)
    WEIGHTED_HYBRID = "weighted_hybrid"  # ê°€ì¤‘ì¹˜ ê¸°ë°˜ í•˜ì´ë¸Œë¦¬ë“œ
    ADAPTIVE = "adaptive"                 # ì ì‘í˜• ê°€ì¤‘ì¹˜

class SearchResult:
    """ê²€ìƒ‰ ê²°ê³¼ ë°ì´í„° í´ë˜ìŠ¤"""
    
    def __init__(self, content: str, metadata: Dict[str, Any], score: float, source: str):
        self.content = content
        self.metadata = metadata
        self.score = score
        self.source = source
        self.rank = 0
        self.relevance_score = 0.0
    
    def to_dict(self) -> Dict[str, Any]:
        return {
            'content': self.content,
            'metadata': self.metadata,
            'score': self.score,
            'source': self.source,
            'rank': self.rank,
            'relevance_score': self.relevance_score
        }

@dataclass
class SearchMetrics:
    """ê²€ìƒ‰ ì„±ëŠ¥ ë©”íŠ¸ë¦­"""
    timestamp: float
    query: str
    algorithm: str
    search_time: float
    results_count: int
    avg_score: float
    top_score: float
    user_feedback: Optional[float] = None
    relevance_score: float = 0.0
    
    def __post_init__(self):
        if self.timestamp is None:
            self.timestamp = time.time()

class WeightOptimizer:
    """ë™ì  ê°€ì¤‘ì¹˜ ìµœì í™” ì‹œìŠ¤í…œ"""
    
    def __init__(self):
        self.weights_history = deque(maxlen=1000)
        self.performance_history = deque(maxlen=1000)
        self.current_weights = {
            'vector_weight': 0.7,
            'bm25_weight': 0.3,
            'rerank_weight': 0.5
        }
        self.learning_rate = 0.01
        self.exploration_rate = 0.1
    
    def update_weights(self, performance_metrics: List[SearchMetrics]) -> Dict[str, float]:
        """ì„±ëŠ¥ ë©”íŠ¸ë¦­ì„ ê¸°ë°˜ìœ¼ë¡œ ê°€ì¤‘ì¹˜ ì—…ë°ì´íŠ¸"""
        if not performance_metrics:
            return self.current_weights
        
        # ìµœê·¼ ì„±ëŠ¥ ê³„ì‚°
        recent_metrics = performance_metrics[-100:]  # ìµœê·¼ 100ê°œ
        avg_relevance = np.mean([m.relevance_score for m in recent_metrics if m.relevance_score > 0])
        avg_search_time = np.mean([m.search_time for m in recent_metrics])
        
        # ê°€ì¤‘ì¹˜ ì¡°ì • ë°©í–¥ ê²°ì •
        if avg_relevance > 0.8:  # ë†’ì€ ê´€ë ¨ì„±
            # í˜„ì¬ ê°€ì¤‘ì¹˜ ìœ ì§€í•˜ë©´ì„œ ë¯¸ì„¸ ì¡°ì •
            self.current_weights['vector_weight'] += self.learning_rate * 0.1
            self.current_weights['bm25_weight'] -= self.learning_rate * 0.1
        elif avg_relevance < 0.6:  # ë‚®ì€ ê´€ë ¨ì„±
            # ë” ê· í˜•ì¡íŒ ê°€ì¤‘ì¹˜ë¡œ ì¡°ì •
            self.current_weights['vector_weight'] = 0.6
            self.current_weights['bm25_weight'] = 0.4
        
        # íƒìƒ‰ì  ì¡°ì • (10% í™•ë¥ )
        if random.random() < self.exploration_rate:
            self._explore_new_weights()
        
        # ê°€ì¤‘ì¹˜ ì •ê·œí™”
        total_weight = sum(self.current_weights.values())
        self.current_weights = {k: v/total_weight for k, v in self.current_weights.items()}
        
        # íˆìŠ¤í† ë¦¬ ì €ì¥
        self.weights_history.append({
            'timestamp': time.time(),
            'weights': self.current_weights.copy(),
            'performance': {'relevance': avg_relevance, 'search_time': avg_search_time}
        })
        
        return self.current_weights
    
    def _explore_new_weights(self):
        """ìƒˆë¡œìš´ ê°€ì¤‘ì¹˜ ì¡°í•© íƒìƒ‰"""
        # ê°€ìš°ì‹œì•ˆ ë…¸ì´ì¦ˆ ì¶”ê°€
        noise = np.random.normal(0, 0.05)
        self.current_weights['vector_weight'] += noise
        self.current_weights['bm25_weight'] -= noise
        
        # ê²½ê³„ ì œí•œ
        self.current_weights['vector_weight'] = max(0.3, min(0.9, self.current_weights['vector_weight']))
        self.current_weights['bm25_weight'] = max(0.1, min(0.7, self.current_weights['bm25_weight']))

class ABTestFramework:
    """A/B í…ŒìŠ¤íŠ¸ í”„ë ˆì„ì›Œí¬"""
    
    def __init__(self):
        self.experiments = {}
        self.user_assignments = {}
        self.results = defaultdict(list)
    
    def create_experiment(self, experiment_id: str, variants: List[Dict[str, Any]], 
                         traffic_split: List[float] = None) -> bool:
        """ìƒˆë¡œìš´ A/B í…ŒìŠ¤íŠ¸ ì‹¤í—˜ ìƒì„±"""
        try:
            if traffic_split is None:
                traffic_split = [1.0 / len(variants)] * len(variants)
            
            if abs(sum(traffic_split) - 1.0) > 0.01:
                raise ValueError("Traffic split must sum to 1.0")
            
            self.experiments[experiment_id] = {
                'variants': variants,
                'traffic_split': traffic_split,
                'start_time': time.time(),
                'status': 'active'
            }
            
            logger.info(f"A/B í…ŒìŠ¤íŠ¸ ì‹¤í—˜ ìƒì„±: {experiment_id}")
            return True
            
        except Exception as e:
            logger.error(f"A/B í…ŒìŠ¤íŠ¸ ì‹¤í—˜ ìƒì„± ì‹¤íŒ¨: {e}")
            return False
    
    def get_variant(self, experiment_id: str, user_id: str) -> Optional[Dict[str, Any]]:
        """ì‚¬ìš©ìì—ê²Œ í…ŒìŠ¤íŠ¸ ë³€í˜• í• ë‹¹"""
        if experiment_id not in self.experiments:
            return None
        
        experiment = self.experiments[experiment_id]
        if experiment['status'] != 'active':
            return None
        
        # ì‚¬ìš©ì í• ë‹¹ í™•ì¸
        if user_id not in self.user_assignments:
            # ìƒˆë¡œìš´ ì‚¬ìš©ì í• ë‹¹
            variant_index = self._assign_variant(experiment['traffic_split'])
            self.user_assignments[user_id] = variant_index
        
        variant_index = self.user_assignments[user_id]
        return experiment['variants'][variant_index]
    
    def _assign_variant(self, traffic_split: List[float]) -> int:
        """íŠ¸ë˜í”½ ë¶„í• ì— ë”°ë¥¸ ë³€í˜• í• ë‹¹"""
        rand = random.random()
        cumulative = 0
        
        for i, split in enumerate(traffic_split):
            cumulative += split
            if rand <= cumulative:
                return i
        
        return len(traffic_split) - 1
    
    def record_result(self, experiment_id: str, user_id: str, variant_index: int, 
                     metrics: Dict[str, Any]) -> bool:
        """A/B í…ŒìŠ¤íŠ¸ ê²°ê³¼ ê¸°ë¡"""
        try:
            result = {
                'timestamp': time.time(),
                'user_id': user_id,
                'variant_index': variant_index,
                'metrics': metrics
            }
            
            self.results[experiment_id].append(result)
            logger.debug(f"A/B í…ŒìŠ¤íŠ¸ ê²°ê³¼ ê¸°ë¡: {experiment_id} - {user_id}")
            return True
            
        except Exception as e:
            logger.error(f"A/B í…ŒìŠ¤íŠ¸ ê²°ê³¼ ê¸°ë¡ ì‹¤íŒ¨: {e}")
            return False
    
    def get_experiment_results(self, experiment_id: str) -> Dict[str, Any]:
        """A/B í…ŒìŠ¤íŠ¸ ê²°ê³¼ ë¶„ì„"""
        if experiment_id not in self.results:
            return {}
        
        results = self.results[experiment_id]
        if not results:
            return {}
        
        # ë³€í˜•ë³„ ì„±ëŠ¥ ë¶„ì„
        variant_performance = defaultdict(list)
        for result in results:
            variant_index = result['variant_index']
            variant_performance[variant_index].append(result['metrics'])
        
        analysis = {}
        for variant_index, metrics_list in variant_performance.items():
            if metrics_list:
                # í‰ê·  ì„±ëŠ¥ ê³„ì‚°
                avg_relevance = np.mean([m.get('relevance_score', 0) for m in metrics_list])
                avg_search_time = np.mean([m.get('search_time', 0) for m in metrics_list])
                avg_user_feedback = np.mean([m.get('user_feedback', 0) for m in metrics_list if m.get('user_feedback')])
                
                analysis[f'variant_{variant_index}'] = {
                    'count': len(metrics_list),
                    'avg_relevance': round(avg_relevance, 3),
                    'avg_search_time': round(avg_search_time, 3),
                    'avg_user_feedback': round(avg_user_feedback, 3) if avg_user_feedback > 0 else None
                }
        
        return analysis

class PerformanceAnalyzer:
    """ì„±ëŠ¥ ë³‘ëª© ì§€ì  ë¶„ì„ ë„êµ¬"""
    
    def __init__(self):
        self.bottleneck_history = deque(maxlen=1000)
        self.thresholds = {
            'search_time': 1.0,      # 1ì´ˆ ì´ìƒì´ë©´ ë³‘ëª©
            'relevance_score': 0.6,   # 0.6 ì´í•˜ë©´ ë‚®ì€ í’ˆì§ˆ
            'results_count': 2        # 2ê°œ ì´í•˜ë©´ ë¶€ì¡±í•œ ê²°ê³¼
        }
    
    def analyze_bottleneck(self, metrics: SearchMetrics) -> Dict[str, Any]:
        """ì„±ëŠ¥ ë³‘ëª© ì§€ì  ë¶„ì„"""
        bottlenecks = []
        
        # ê²€ìƒ‰ ì‹œê°„ ë³‘ëª©
        if metrics.search_time > self.thresholds['search_time']:
            bottlenecks.append({
                'type': 'search_time',
                'severity': 'high' if metrics.search_time > 2.0 else 'medium',
                'value': metrics.search_time,
                'threshold': self.thresholds['search_time'],
                'suggestion': 'ê²€ìƒ‰ ì•Œê³ ë¦¬ì¦˜ ìµœì í™” ë˜ëŠ” ì¸ë±ìŠ¤ ê°œì„  í•„ìš”'
            })
        
        # ê´€ë ¨ì„± ì ìˆ˜ ë³‘ëª©
        if metrics.relevance_score < self.thresholds['relevance_score']:
            bottlenecks.append({
                'type': 'relevance_score',
                'severity': 'high' if metrics.relevance_score < 0.4 else 'medium',
                'value': metrics.relevance_score,
                'threshold': self.thresholds['relevance_score'],
                'suggestion': 'ì„ë² ë”© ëª¨ë¸ ê°œì„  ë˜ëŠ” ê²€ìƒ‰ íŒŒë¼ë¯¸í„° ì¡°ì • í•„ìš”'
            })
        
        # ê²°ê³¼ ìˆ˜ ë³‘ëª©
        if metrics.results_count < self.thresholds['results_count']:
            bottlenecks.append({
                'type': 'results_count',
                'severity': 'medium',
                'value': metrics.results_count,
                'threshold': self.thresholds['results_count'],
                'suggestion': 'ê²€ìƒ‰ ë²”ìœ„ í™•ëŒ€ ë˜ëŠ” ì²­í‚¹ ì „ëµ ê°œì„  í•„ìš”'
            })
        
        # ë³‘ëª© íˆìŠ¤í† ë¦¬ ì €ì¥
        if bottlenecks:
            self.bottleneck_history.append({
                'timestamp': metrics.timestamp,
                'query': metrics.query,
                'algorithm': metrics.algorithm,
                'bottlenecks': bottlenecks
            })
        
        return {
            'has_bottleneck': len(bottlenecks) > 0,
            'bottlenecks': bottlenecks,
            'overall_severity': self._calculate_overall_severity(bottlenecks)
        }
    
    def _calculate_overall_severity(self, bottlenecks: List[Dict[str, Any]]) -> str:
        """ì „ì²´ ë³‘ëª© ì‹¬ê°ë„ ê³„ì‚°"""
        if not bottlenecks:
            return 'none'
        
        severity_scores = {'low': 1, 'medium': 2, 'high': 3}
        max_severity = max(severity_scores.get(b['severity'], 0) for b in bottlenecks)
        
        if max_severity >= 3:
            return 'critical'
        elif max_severity >= 2:
            return 'high'
        else:
            return 'low'
    
    def get_bottleneck_trends(self, hours: int = 24) -> Dict[str, Any]:
        """ë³‘ëª© íŠ¸ë Œë“œ ë¶„ì„"""
        cutoff_time = time.time() - (hours * 3600)
        recent_bottlenecks = [b for b in self.bottleneck_history if b['timestamp'] > cutoff_time]
        
        if not recent_bottlenecks:
            return {}
        
        # ë³‘ëª© ìœ í˜•ë³„ ë°œìƒ ë¹ˆë„
        bottleneck_types = defaultdict(int)
        algorithm_bottlenecks = defaultdict(int)
        
        for bottleneck in recent_bottlenecks:
            for b in bottleneck['bottlenecks']:
                bottleneck_types[b['type']] += 1
            algorithm_bottlenecks[bottleneck['algorithm']] += 1
        
        return {
            'total_bottlenecks': len(recent_bottlenecks),
            'bottleneck_types': dict(bottleneck_types),
            'algorithm_bottlenecks': dict(algorithm_bottlenecks),
            'trend': self._analyze_trend(recent_bottlenecks)
        }
    
    def _analyze_trend(self, bottlenecks: List[Dict[str, Any]]) -> str:
        """ë³‘ëª© íŠ¸ë Œë“œ ë¶„ì„"""
        if len(bottlenecks) < 2:
            return 'insufficient_data'
        
        # ì‹œê°„ìˆœìœ¼ë¡œ ì •ë ¬
        sorted_bottlenecks = sorted(bottlenecks, key=lambda x: x['timestamp'])
        
        # ìµœê·¼ 1/3ê³¼ ì´ì „ 1/3 ë¹„êµ
        third = len(sorted_bottlenecks) // 3
        recent = sorted_bottlenecks[-third:]
        previous = sorted_bottlenecks[:third]
        
        recent_count = len(recent)
        previous_count = len(previous)
        
        if recent_count < previous_count * 0.7:
            return 'improving'
        elif recent_count > previous_count * 1.3:
            return 'worsening'
        else:
            return 'stable'

class AdvancedSearchEngine:
    """ê³ ê¸‰ ê²€ìƒ‰ ì—”ì§„"""
    
    def __init__(self):
        self.weight_optimizer = WeightOptimizer()
        self.ab_test_framework = ABTestFramework()
        self.performance_analyzer = PerformanceAnalyzer()
        self.search_metrics_history = deque(maxlen=1000)
        
        # ê¸°ë³¸ A/B í…ŒìŠ¤íŠ¸ ì‹¤í—˜ ìƒì„±
        self._setup_default_experiments()
    
    def _setup_default_experiments(self):
        """ê¸°ë³¸ A/B í…ŒìŠ¤íŠ¸ ì‹¤í—˜ ì„¤ì •"""
        # ê²€ìƒ‰ ì•Œê³ ë¦¬ì¦˜ ë¹„êµ ì‹¤í—˜
        self.ab_test_framework.create_experiment(
            experiment_id="search_algorithm_comparison",
            variants=[
                {'algorithm': 'vector_only', 'description': 'ë²¡í„° ê²€ìƒ‰ë§Œ ì‚¬ìš©'},
                {'algorithm': 'hybrid', 'description': 'í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰'},
                {'algorithm': 'weighted_hybrid', 'description': 'ê°€ì¤‘ì¹˜ ê¸°ë°˜ í•˜ì´ë¸Œë¦¬ë“œ'}
            ],
            traffic_split=[0.33, 0.33, 0.34]
        )
    
    def search(self, query: str, user_id: str = None, 
               algorithm: str = None, use_ab_test: bool = True) -> List[SearchResult]:
        """ê³ ê¸‰ ê²€ìƒ‰ ì‹¤í–‰"""
        start_time = time.time()
        
        # A/B í…ŒìŠ¤íŠ¸ ë³€í˜• í• ë‹¹
        if use_ab_test and user_id:
            variant = self.ab_test_framework.get_variant("search_algorithm_comparison", user_id)
            if variant:
                algorithm = variant['algorithm']
        
        # ì•Œê³ ë¦¬ì¦˜ë³„ ê²€ìƒ‰ ì‹¤í–‰
        if algorithm == 'vector_only':
            results = self._vector_search(query)
        elif algorithm == 'bm25_only':
            results = self._bm25_search(query)
        elif algorithm == 'hybrid':
            results = self._hybrid_search(query)
        elif algorithm == 'weighted_hybrid':
            results = self._weighted_hybrid_search(query)
        else:
            # ê¸°ë³¸ê°’: ê°€ì¤‘ì¹˜ ê¸°ë°˜ í•˜ì´ë¸Œë¦¬ë“œ
            results = self._weighted_hybrid_search(query)
        
        search_time = time.time() - start_time
        
        # ì„±ëŠ¥ ë©”íŠ¸ë¦­ ìƒì„±
        metrics = SearchMetrics(
            timestamp=start_time,
            query=query,
            algorithm=algorithm or 'weighted_hybrid',
            search_time=search_time,
            results_count=len(results),
            avg_score=np.mean([r.score for r in results]) if results else 0,
            top_score=results[0].score if results else 0
        )
        
        # ì„±ëŠ¥ ë¶„ì„
        bottleneck_analysis = self.performance_analyzer.analyze_bottleneck(metrics)
        
        # ë©”íŠ¸ë¦­ íˆìŠ¤í† ë¦¬ ì €ì¥
        self.search_metrics_history.append(metrics)
        
        # A/B í…ŒìŠ¤íŠ¸ ê²°ê³¼ ê¸°ë¡
        if user_id and use_ab_test:
            variant_index = self._get_variant_index(algorithm)
            self.ab_test_framework.record_result(
                "search_algorithm_comparison",
                user_id,
                variant_index,
                {
                    'search_time': search_time,
                    'results_count': len(results),
                    'avg_score': metrics.avg_score,
                    'top_score': metrics.top_score,
                    'bottleneck_severity': bottleneck_analysis['overall_severity']
                }
            )
        
        # ê°€ì¤‘ì¹˜ ìµœì í™”
        self.weight_optimizer.update_weights(list(self.search_metrics_history))
        
        return results
    
    def _vector_search(self, query: str) -> List[SearchResult]:
        """ë²¡í„° ê²€ìƒ‰ (ì‹¤ì œ êµ¬í˜„ì€ ê¸°ì¡´ ì‹œìŠ¤í…œê³¼ ì—°ë™)"""
        # ì„ì‹œ êµ¬í˜„ - ì‹¤ì œë¡œëŠ” ê¸°ì¡´ ë²¡í„° ê²€ìƒ‰ ì‚¬ìš©
        return [
            SearchResult(
                content=f"ë²¡í„° ê²€ìƒ‰ ê²°ê³¼: {query}",
                metadata={'source': 'vector_search'},
                score=0.85,
                source='vector_search'
            )
        ]
    
    def _bm25_search(self, query: str) -> List[SearchResult]:
        """BM25 ê²€ìƒ‰ (ì‹¤ì œ êµ¬í˜„ì€ ê¸°ì¡´ ì‹œìŠ¤í…œê³¼ ì—°ë™)"""
        # ì„ì‹œ êµ¬í˜„ - ì‹¤ì œë¡œëŠ” ê¸°ì¡´ BM25 ê²€ìƒ‰ ì‚¬ìš©
        return [
            SearchResult(
                content=f"BM25 ê²€ìƒ‰ ê²°ê³¼: {query}",
                metadata={'source': 'bm25_search'},
                score=0.75,
                source='bm25_search'
            )
        ]
    
    def _hybrid_search(self, query: str) -> List[SearchResult]:
        """í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰"""
        vector_results = self._vector_search(query)
        bm25_results = self._bm25_search(query)
        
        # ê²°ê³¼ ë³‘í•© ë° ì¬ë­í‚¹
        all_results = vector_results + bm25_results
        all_results.sort(key=lambda x: x.score, reverse=True)
        
        return all_results[:5]  # ìƒìœ„ 5ê°œ ê²°ê³¼
    
    def _weighted_hybrid_search(self, query: str) -> List[SearchResult]:
        """ê°€ì¤‘ì¹˜ ê¸°ë°˜ í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰"""
        weights = self.weight_optimizer.current_weights
        
        vector_results = self._vector_search(query)
        bm25_results = self._bm25_search(query)
        
        # ê°€ì¤‘ì¹˜ ì ìš©
        for result in vector_results:
            result.score *= weights['vector_weight']
        
        for result in bm25_results:
            result.score *= weights['bm25_weight']
        
        # ê²°ê³¼ ë³‘í•© ë° ì¬ë­í‚¹
        all_results = vector_results + bm25_results
        all_results.sort(key=lambda x: x.score, reverse=True)
        
        return all_results[:5]
    
    def _get_variant_index(self, algorithm: str) -> int:
        """ì•Œê³ ë¦¬ì¦˜ì— ë”°ë¥¸ ë³€í˜• ì¸ë±ìŠ¤ ë°˜í™˜"""
        algorithm_mapping = {
            'vector_only': 0,
            'hybrid': 1,
            'weighted_hybrid': 2
        }
        return algorithm_mapping.get(algorithm, 2)
    
    def get_performance_insights(self) -> Dict[str, Any]:
        """ì„±ëŠ¥ ì¸ì‚¬ì´íŠ¸ ì œê³µ"""
        return {
            'current_weights': self.weight_optimizer.current_weights,
            'bottleneck_trends': self.performance_analyzer.get_bottleneck_trends(),
            'ab_test_results': self.ab_test_framework.get_experiment_results("search_algorithm_comparison"),
            'search_metrics_summary': self._get_metrics_summary()
        }
    
    def _get_metrics_summary(self) -> Dict[str, Any]:
        """ê²€ìƒ‰ ë©”íŠ¸ë¦­ ìš”ì•½"""
        if not self.search_metrics_history:
            return {}
        
        recent_metrics = list(self.search_metrics_history)[-100:]  # ìµœê·¼ 100ê°œ
        
        return {
            'total_searches': len(self.search_metrics_history),
            'recent_searches': len(recent_metrics),
            'avg_search_time': np.mean([m.search_time for m in recent_metrics]),
            'avg_results_count': np.mean([m.results_count for m in recent_metrics]),
            'algorithm_distribution': self._get_algorithm_distribution(recent_metrics)
        }
    
    def _get_algorithm_distribution(self, metrics: List[SearchMetrics]) -> Dict[str, int]:
        """ì•Œê³ ë¦¬ì¦˜ë³„ ì‚¬ìš© ë¶„í¬"""
        distribution = defaultdict(int)
        for metric in metrics:
            distribution[metric.algorithm] += 1
        return dict(distribution)

# ì „ì—­ ê³ ê¸‰ ê²€ìƒ‰ ì—”ì§„ ì¸ìŠ¤í„´ìŠ¤
advanced_search_engine = AdvancedSearchEngine()

def get_search_insights() -> Dict[str, Any]:
    """ê²€ìƒ‰ ì„±ëŠ¥ ì¸ì‚¬ì´íŠ¸ ì¡°íšŒ"""
    return advanced_search_engine.get_performance_insights()

if __name__ == "__main__":
    # í…ŒìŠ¤íŠ¸ ì½”ë“œ
    print("ğŸ” DevDesk-RAG ê³ ê¸‰ ê²€ìƒ‰ ì•Œê³ ë¦¬ì¦˜ í…ŒìŠ¤íŠ¸")
    
    # ê²€ìƒ‰ í…ŒìŠ¤íŠ¸
    results = advanced_search_engine.search(
        query="DevDesk-RAG ì‹œìŠ¤í…œì˜ ì„±ëŠ¥ì€ ì–´ë–¨ê¹Œìš”?",
        user_id="test_user_123",
        algorithm="weighted_hybrid"
    )
    
    print(f"âœ… ê²€ìƒ‰ ì™„ë£Œ: {len(results)}ê°œ ê²°ê³¼")
    
    # ì„±ëŠ¥ ì¸ì‚¬ì´íŠ¸ ì¡°íšŒ
    insights = get_search_insights()
    print(f"ğŸ“Š ì„±ëŠ¥ ì¸ì‚¬ì´íŠ¸: {insights}")
    
    print("ğŸ‰ ê³ ê¸‰ ê²€ìƒ‰ ì•Œê³ ë¦¬ì¦˜ í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")
