#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
DevDesk-RAG íŠ¹í™” LoRA ëª¨ë¸ í›ˆë ¨ ìŠ¤í¬ë¦½íŠ¸
EXAONE ëª¨ë¸ì„ ê¸°ë°˜ìœ¼ë¡œ DevDesk-RAG ì‹œìŠ¤í…œì— íŠ¹í™”ëœ LoRA ì–´ëŒ‘í„°ë¥¼ í›ˆë ¨í•©ë‹ˆë‹¤.
"""

import os
import torch
import json
from typing import List, Dict
from datasets import Dataset
from transformers import (
    AutoModelForCausalLM, 
    AutoTokenizer, 
    TrainingArguments,
    Trainer,
    DataCollatorForLanguageModeling
)
from peft import LoraConfig, get_peft_model, TaskType
import logging

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

class DevDeskRAGLoRATrainer:
    """DevDesk-RAG íŠ¹í™” LoRA í›ˆë ¨ê¸°"""
    
    def __init__(self, model_name: str = "exaone3.5:7.8b"):
        self.model_name = model_name
        self.device = self._setup_device()
        self.model = None
        self.tokenizer = None
        self.lora_config = None
        
    def _setup_device(self) -> torch.device:
        """í›ˆë ¨ ë””ë°”ì´ìŠ¤ ì„¤ì •"""
        if torch.backends.mps.is_available():
            device = torch.device("mps")
            logger.info("ğŸš€ Apple Silicon MPS (Metal) ì‚¬ìš©")
        elif torch.cuda.is_available():
            device = torch.device("cuda")
            logger.info("ğŸš€ NVIDIA GPU CUDA ì‚¬ìš©")
        else:
            device = torch.device("cpu")
            logger.info("ğŸ’» CPU ì‚¬ìš© (ëŠë¦´ ìˆ˜ ìˆìŒ)")
        
        return device
    
    def load_model_and_tokenizer(self):
        """ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì € ë¡œë“œ"""
        logger.info(f"ğŸ“¥ ëª¨ë¸ ë¡œë”© ì¤‘: {self.model_name}")
        
        try:
            # í† í¬ë‚˜ì´ì € ë¡œë“œ
            self.tokenizer = AutoTokenizer.from_pretrained(
                self.model_name,
                trust_remote_code=True,
                padding_side="right"
            )
            
            # íŒ¨ë”© í† í° ì„¤ì •
            if self.tokenizer.pad_token is None:
                self.tokenizer.pad_token = self.tokenizer.eos_token
            
            # ëª¨ë¸ ë¡œë“œ
            self.model = AutoModelForCausalLM.from_pretrained(
                self.model_name,
                trust_remote_code=True,
                torch_dtype=torch.float16 if self.device.type != "cpu" else torch.float32,
                device_map="auto" if self.device.type != "cpu" else None
            )
            
            # ëª¨ë¸ì„ ë””ë°”ì´ìŠ¤ë¡œ ì´ë™
            if self.device.type == "cpu":
                self.model = self.model.to(self.device)
            
            logger.info("âœ… ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì € ë¡œë“œ ì™„ë£Œ")
            
        except Exception as e:
            logger.error(f"âŒ ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨: {e}")
            raise
    
    def setup_lora_config(self):
        """LoRA ì„¤ì • êµ¬ì„±"""
        logger.info("ğŸ”§ LoRA ì„¤ì • êµ¬ì„± ì¤‘...")
        
        self.lora_config = LoraConfig(
            r=16,  # LoRA ë­í¬
            lora_alpha=32,
            target_modules=["q_proj", "v_proj", "k_proj", "o_proj"],
            lora_dropout=0.1,
            bias="none",
            task_type=TaskType.CAUSAL_LM
        )
        
        logger.info("âœ… LoRA ì„¤ì • ì™„ë£Œ")
    
    def apply_lora_to_model(self):
        """ëª¨ë¸ì— LoRA ì ìš©"""
        logger.info("ğŸ”— LoRAë¥¼ ëª¨ë¸ì— ì ìš© ì¤‘...")
        
        try:
            self.model = get_peft_model(self.model, self.lora_config)
            self.model.print_trainable_parameters()
            logger.info("âœ… LoRA ì ìš© ì™„ë£Œ")
            
        except Exception as e:
            logger.error(f"âŒ LoRA ì ìš© ì‹¤íŒ¨: {e}")
            raise
    
    def load_training_data(self, filename: str = "devdesk_rag_training.jsonl") -> Dataset:
        """í›ˆë ¨ ë°ì´í„° ë¡œë“œ"""
        logger.info(f"ğŸ“š í›ˆë ¨ ë°ì´í„° ë¡œë“œ ì¤‘: {filename}")
        
        try:
            data = []
            with open(filename, 'r', encoding='utf-8') as f:
                for line in f:
                    data.append(json.loads(line.strip()))
            
            # ë°ì´í„°ì…‹ ìƒì„±
            dataset = Dataset.from_list(data)
            logger.info(f"âœ… {len(dataset)}ê°œ ìƒ˜í”Œ ë¡œë“œ ì™„ë£Œ")
            
            return dataset
            
        except Exception as e:
            logger.error(f"âŒ í›ˆë ¨ ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨: {e}")
            raise
    
    def tokenize_function(self, examples):
        """ë°ì´í„° í† í¬ë‚˜ì´ì§•"""
        return self.tokenizer(
            examples["text"],
            truncation=True,
            padding=True,
            max_length=1024,
            return_tensors="pt"
        )
    
    def prepare_dataset(self, dataset: Dataset) -> Dataset:
        """ë°ì´í„°ì…‹ ì „ì²˜ë¦¬"""
        logger.info("ğŸ”§ ë°ì´í„°ì…‹ ì „ì²˜ë¦¬ ì¤‘...")
        
        # í† í¬ë‚˜ì´ì§•
        tokenized_dataset = dataset.map(
            self.tokenize_function,
            batched=True,
            remove_columns=dataset.column_names
        )
        
        logger.info("âœ… ë°ì´í„°ì…‹ ì „ì²˜ë¦¬ ì™„ë£Œ")
        return tokenized_dataset
    
    def setup_training_args(self, output_dir: str = "./devdesk_rag_lora") -> TrainingArguments:
        """í›ˆë ¨ ì¸ì ì„¤ì •"""
        logger.info("âš™ï¸ í›ˆë ¨ ì¸ì ì„¤ì • ì¤‘...")
        
        training_args = TrainingArguments(
            output_dir=output_dir,
            num_train_epochs=3,
            per_device_train_batch_size=1,
            gradient_accumulation_steps=4,
            learning_rate=2e-4,
            warmup_steps=100,
            logging_steps=10,
            save_steps=500,
            evaluation_strategy="steps",
            eval_steps=500,
            save_total_limit=2,
            load_best_model_at_end=True,
            report_to="none",  # wandb ë¹„í™œì„±í™”
            remove_unused_columns=False,
            dataloader_pin_memory=False,
            dataloader_num_workers=0,
            fp16=self.device.type != "cpu",
            bf16=False,
            gradient_checkpointing=True,
            optim="adamw_torch",
            lr_scheduler_type="cosine",
            weight_decay=0.01,
            max_grad_norm=1.0,
            seed=42,
            data_seed=42,
            group_by_length=True,
            length_column_name="length",
            ignore_data_skip=False,
            dataloader_drop_last=False,
            eval_delay=0,
            save_on_each_node=False,
            auto_find_batch_size=False,
            full_determinism=False,
            ddp_find_unused_parameters=None,
            dataloader_pin_memory_device="",
            torch_compile=False,
            torch_compile_backend="inductor",
            torch_compile_mode="default",
            dataloader_prefetch_factor=None,
            dataloader_persistent_workers=False,
            dataloader_prefetch_factor=None,
            dataloader_persistent_workers=False,
        )
        
        logger.info("âœ… í›ˆë ¨ ì¸ì ì„¤ì • ì™„ë£Œ")
        return training_args
    
    def train(self, dataset: Dataset, output_dir: str = "./devdesk_rag_lora"):
        """LoRA ëª¨ë¸ í›ˆë ¨ ì‹¤í–‰"""
        logger.info("ğŸš€ LoRA í›ˆë ¨ ì‹œì‘!")
        
        try:
            # 1. ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì € ë¡œë“œ
            self.load_model_and_tokenizer()
            
            # 2. LoRA ì„¤ì •
            self.setup_lora_config()
            
            # 3. LoRA ì ìš©
            self.apply_lora_to_model()
            
            # 4. ë°ì´í„°ì…‹ ì „ì²˜ë¦¬
            processed_dataset = self.prepare_dataset(dataset)
            
            # 5. í›ˆë ¨ ì¸ì ì„¤ì •
            training_args = self.setup_training_args(output_dir)
            
            # 6. ë°ì´í„° ì½œë ˆì´í„° ì„¤ì •
            data_collator = DataCollatorForLanguageModeling(
                tokenizer=self.tokenizer,
                mlm=False
            )
            
            # 7. í›ˆë ¨ê¸° ìƒì„± ë° í›ˆë ¨ ì‹¤í–‰
            trainer = Trainer(
                model=self.model,
                args=training_args,
                train_dataset=processed_dataset,
                data_collator=data_collator,
                tokenizer=self.tokenizer
            )
            
            logger.info("ğŸ”¥ í›ˆë ¨ ì‹¤í–‰ ì¤‘...")
            trainer.train()
            
            # 8. ëª¨ë¸ ì €ì¥
            logger.info(f"ğŸ’¾ ëª¨ë¸ ì €ì¥ ì¤‘: {output_dir}")
            trainer.save_model()
            self.tokenizer.save_pretrained(output_dir)
            
            logger.info("ğŸ‰ LoRA í›ˆë ¨ ì™„ë£Œ!")
            return True
            
        except Exception as e:
            logger.error(f"âŒ í›ˆë ¨ ì‹¤íŒ¨: {e}")
            raise

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    logger.info("ğŸš€ DevDesk-RAG íŠ¹í™” LoRA í›ˆë ¨ ì‹œì‘!")
    
    try:
        # 1. í›ˆë ¨ê¸° ì´ˆê¸°í™”
        trainer = DevDeskRAGLoRATrainer()
        
        # 2. í›ˆë ¨ ë°ì´í„° ë¡œë“œ
        dataset = trainer.load_training_data()
        
        # 3. LoRA í›ˆë ¨ ì‹¤í–‰
        success = trainer.train(dataset)
        
        if success:
            logger.info("ğŸ‰ DevDesk-RAG íŠ¹í™” LoRA í›ˆë ¨ì´ ì„±ê³µì ìœ¼ë¡œ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!")
            logger.info("ğŸ“ í›ˆë ¨ëœ ëª¨ë¸: ./devdesk_rag_lora/")
            logger.info("ğŸš€ ì´ì œ Ollamaì— ì ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤!")
        
    except Exception as e:
        logger.error(f"âŒ í›ˆë ¨ ì‹¤íŒ¨: {e}")
        return False
    
    return True

if __name__ == "__main__":
    main()
